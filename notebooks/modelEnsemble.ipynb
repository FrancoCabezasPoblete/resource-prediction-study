{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "#from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CUDA\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if DEVICE.type == 'cuda':\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "sampler = TPESampler(seed=seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_to_MiB(row):\n",
    "\t# verify if has string ' MiB'\n",
    "\tif 'MiB' in str(row):\n",
    "\t\trow = row.replace(' MiB', '')\n",
    "\t\trow = float(row)\n",
    "\telse:\n",
    "\t\trow = float(row) / np.power(2, 20)\n",
    "\treturn row\n",
    "\n",
    "\n",
    "def MHz_to_GHz(row):\n",
    "\t# verify if has string ' GHz'\n",
    "\tif 'GHz' in str(row):\n",
    "\t\trow = row.replace(' GHz', '')\n",
    "\t\t# convert to float\n",
    "\t\trow = float(row)\n",
    "\telse:\n",
    "\t\trow = row.replace(' MHz', '')\n",
    "\t\trow = float(row) / 1000\n",
    "\treturn row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('../results_new/execution_time.csv')\n",
    "results_savio_df = pd.read_csv('../results_savio_new/execution_time.csv')\n",
    "results_df = pd.concat([results_df, results_savio_df], ignore_index=True)\n",
    "# preprocessing\n",
    "results_df['total_cpu_usage'] = results_df['total_cpu_usage'].str.replace('%', '').astype(float) / 100\n",
    "results_df['max_ram_usage'] = results_df['max_ram_usage'] / 1024\n",
    "results_df['l2_cache_size'] = results_df['l2_cache_size'].apply(bits_to_MiB)\n",
    "results_df['l3_cache_size'] = results_df['l3_cache_size'].apply(bits_to_MiB)\n",
    "results_df['ghz_actual_friendly'] = results_df['hz_actual_friendly'].apply(MHz_to_GHz)\n",
    "results_df['ghz_advertised_friendly'] = results_df['hz_advertised_friendly'].str.replace('GHz', '').astype(float)\n",
    "results_df = results_df.drop(columns=['hz_actual_friendly', 'hz_advertised_friendly', 'arch', 'vendor_id_raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_time</th>\n",
       "      <th>total_cpu_usage</th>\n",
       "      <th>max_ram_usage</th>\n",
       "      <th>brand_raw</th>\n",
       "      <th>count</th>\n",
       "      <th>l2_cache_size</th>\n",
       "      <th>l3_cache_size</th>\n",
       "      <th>l2_cache_line_size</th>\n",
       "      <th>l2_cache_associativity</th>\n",
       "      <th>benchmark</th>\n",
       "      <th>ghz_actual_friendly</th>\n",
       "      <th>ghz_advertised_friendly</th>\n",
       "      <th>total_time_target</th>\n",
       "      <th>brand_raw_target</th>\n",
       "      <th>count_target</th>\n",
       "      <th>l2_cache_size_target</th>\n",
       "      <th>l3_cache_size_target</th>\n",
       "      <th>l2_cache_line_size_target</th>\n",
       "      <th>l2_cache_associativity_target</th>\n",
       "      <th>ghz_advertised_friendly_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.47</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1436.714844</td>\n",
       "      <td>Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz</td>\n",
       "      <td>12</td>\n",
       "      <td>1.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "      <td>KNP</td>\n",
       "      <td>4.1729</td>\n",
       "      <td>2.9</td>\n",
       "      <td>45.91</td>\n",
       "      <td>13th Gen Intel(R) Core(TM) i5-1335U</td>\n",
       "      <td>12</td>\n",
       "      <td>7.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1280</td>\n",
       "      <td>7</td>\n",
       "      <td>2.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13.47</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1436.714844</td>\n",
       "      <td>Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz</td>\n",
       "      <td>12</td>\n",
       "      <td>1.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>256</td>\n",
       "      <td>6</td>\n",
       "      <td>KNP</td>\n",
       "      <td>4.1729</td>\n",
       "      <td>2.9</td>\n",
       "      <td>25.77</td>\n",
       "      <td>13th Gen Intel(R) Core(TM) i5-1335U</td>\n",
       "      <td>12</td>\n",
       "      <td>7.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1280</td>\n",
       "      <td>7</td>\n",
       "      <td>2.496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_time  total_cpu_usage  max_ram_usage  \\\n",
       "5       13.47             0.99    1436.714844   \n",
       "6       13.47             0.99    1436.714844   \n",
       "\n",
       "                                  brand_raw  count  l2_cache_size  \\\n",
       "5  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz     12            1.5   \n",
       "6  Intel(R) Core(TM) i5-10400 CPU @ 2.90GHz     12            1.5   \n",
       "\n",
       "   l3_cache_size  l2_cache_line_size  l2_cache_associativity benchmark  \\\n",
       "5           12.0                 256                       6       KNP   \n",
       "6           12.0                 256                       6       KNP   \n",
       "\n",
       "   ghz_actual_friendly  ghz_advertised_friendly  total_time_target  \\\n",
       "5               4.1729                      2.9              45.91   \n",
       "6               4.1729                      2.9              25.77   \n",
       "\n",
       "                      brand_raw_target  count_target  l2_cache_size_target  \\\n",
       "5  13th Gen Intel(R) Core(TM) i5-1335U            12                   7.5   \n",
       "6  13th Gen Intel(R) Core(TM) i5-1335U            12                   7.5   \n",
       "\n",
       "   l3_cache_size_target  l2_cache_line_size_target  \\\n",
       "5                  12.0                       1280   \n",
       "6                  12.0                       1280   \n",
       "\n",
       "   l2_cache_associativity_target  ghz_advertised_friendly_target  \n",
       "5                              7                           2.496  \n",
       "6                              7                           2.496  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the target dataset\n",
    "target_df = results_df[['total_time', 'brand_raw', 'count', 'l2_cache_size', 'l3_cache_size', 'l2_cache_line_size', 'l2_cache_associativity', 'ghz_advertised_friendly', 'benchmark']].copy()\n",
    "# Rename columns to *_target\n",
    "target_df = target_df.rename(columns={\n",
    "    'total_time': 'total_time_target',\n",
    "    'brand_raw': 'brand_raw_target',\n",
    "    'count': 'count_target',\n",
    "    'l2_cache_size': 'l2_cache_size_target',\n",
    "    'l3_cache_size': 'l3_cache_size_target',\n",
    "    'l2_cache_line_size': 'l2_cache_line_size_target',\n",
    "    'l2_cache_associativity': 'l2_cache_associativity_target',\n",
    "    'ghz_advertised_friendly': 'ghz_advertised_friendly_target',\n",
    "})\n",
    "\n",
    "dataset_df = pd.merge(results_df, target_df, how='inner', on='benchmark')\n",
    "dataset_df = dataset_df[dataset_df['brand_raw'] != dataset_df['brand_raw_target']]\n",
    "dataset_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove one computer for testing\n",
    "g_train = dataset_df[(dataset_df['brand_raw'] != '13th Gen Intel(R) Core(TM) i5-1335U') & (dataset_df['brand_raw_target'] != '13th Gen Intel(R) Core(TM) i5-1335U')]\n",
    "g_test = dataset_df[dataset_df['brand_raw_target'] == '13th Gen Intel(R) Core(TM) i5-1335U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_df = dataset_df[dataset_df['benchmark'].isin(['MATRIX_MULT', 'MATRIX_MULT2', 'MATRIX_MULT3'])]\n",
    "# remove one computer for testing\n",
    "mm_train = mm_df[(mm_df['brand_raw'] != '13th Gen Intel(R) Core(TM) i5-1335U') & (mm_df['brand_raw_target'] != '13th Gen Intel(R) Core(TM) i5-1335U')]\n",
    "mm_test = mm_df[mm_df['brand_raw_target'] == '13th Gen Intel(R) Core(TM) i5-1335U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_df = dataset_df[~dataset_df['benchmark'].isin(['MATRIX_MULT', 'MATRIX_MULT2', 'MATRIX_MULT3'])]\n",
    "# remove one computer for testing\n",
    "st_train = st_df[(st_df['brand_raw'] != '13th Gen Intel(R) Core(TM) i5-1335U') & (st_df['brand_raw_target'] != '13th Gen Intel(R) Core(TM) i5-1335U')]\n",
    "st_test = st_df[st_df['brand_raw_target'] == '13th Gen Intel(R) Core(TM) i5-1335U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "g_test = pd.read_csv('csv/g_test.csv')\n",
    "st_test = pd.read_csv('csv/st_test.csv')\n",
    "mm_test = pd.read_csv('csv/mm_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'total_time_target'\n",
    "features = mm_test.columns.copy().drop(target).drop(['benchmark','brand_raw', 'brand_raw_target'])\n",
    "features_st = features.copy().drop(['count', 'count_target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general data\n",
    "## split data\n",
    "X_g_train = g_train[features]\n",
    "y_g_train = g_train[target]\n",
    "\n",
    "X_g_test = g_test[features]\n",
    "y_g_test = g_test[target]\n",
    "\n",
    "## normalize data\n",
    "scaler_g = StandardScaler()\n",
    "X_g_train = scaler_g.fit_transform(X_g_train)\n",
    "X_g_test = scaler_g.transform(X_g_test)\n",
    "\n",
    "## convert to tensor\n",
    "X_g_train_t = torch.tensor(X_g_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_g_test_t = torch.tensor(X_g_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_g_train_t = torch.tensor(y_g_train.values, dtype=torch.float32).view(-1, 1)\n",
    "y_g_test_t = torch.tensor(y_g_test.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single thread data\n",
    "## split data\n",
    "X_st_train = st_train[features_st]\n",
    "y_st_train = st_train[target]\n",
    "\n",
    "X_st_test = st_test[features_st]\n",
    "y_st_test = st_test[target]\n",
    "\n",
    "## normalize data\n",
    "scaler_st = StandardScaler()\n",
    "X_st_train = scaler_st.fit_transform(X_st_train)\n",
    "X_st_test = scaler_st.transform(X_st_test)\n",
    "\n",
    "## convert to tensor\n",
    "X_st_train_t = torch.tensor(X_st_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_st_test_t = torch.tensor(X_st_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_st_train_t = torch.tensor(y_st_train.values, dtype=torch.float32).view(-1, 1)\n",
    "y_st_test_t = torch.tensor(y_st_test.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi thread data\n",
    "## split data\n",
    "X_mm_train = mm_train[features]\n",
    "y_mm_train = mm_train[target]\n",
    "\n",
    "X_mm_test = mm_test[features]\n",
    "y_mm_test = mm_test[target]\n",
    "\n",
    "## normalize data\n",
    "scaler_mm = StandardScaler()\n",
    "X_mm_train = scaler_mm.fit_transform(X_mm_train)\n",
    "X_mm_test = scaler_mm.transform(X_mm_test)\n",
    "\n",
    "## convert to tensor\n",
    "X_mm_train_t = torch.tensor(X_mm_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_mm_test_t = torch.tensor(X_mm_test, dtype=torch.float32).unsqueeze(1)\n",
    "y_mm_train_t = torch.tensor(y_mm_train.values, dtype=torch.float32).view(-1, 1)\n",
    "y_mm_test_t = torch.tensor(y_mm_test.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEVICE.type == 'cuda':\n",
    "\t# move to DEVICE\n",
    "\tX_g_train_t = X_g_train_t.to(DEVICE)\n",
    "\ty_g_train_t = y_g_train_t.to(DEVICE)\n",
    "\tX_g_test_t = X_g_test_t.to(DEVICE)\n",
    "\ty_g_test_t = y_g_test_t.to(DEVICE)\n",
    "\n",
    "\tX_st_train_t = X_st_train_t.to(DEVICE)\n",
    "\ty_st_train_t = y_st_train_t.to(DEVICE)\n",
    "\tX_st_test_t = X_st_test_t.to(DEVICE)\n",
    "\ty_st_test_t = y_st_test_t.to(DEVICE)\n",
    "\n",
    "\tX_mm_train_t = X_mm_train_t.to(DEVICE)\n",
    "\ty_mm_train_t = y_mm_train_t.to(DEVICE)\n",
    "\tX_mm_test_t = X_mm_test_t.to(DEVICE)\n",
    "\ty_mm_test_t = y_mm_test_t.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardModel(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim, dropout=0.1):\n",
    "\t\tsuper(FeedforwardModel, self).__init__()\n",
    "\t\t# layers\n",
    "\t\tself.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.model(x).view(-1,1)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\tdef __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim, dropout=0.1):\n",
    "\t\tsuper(TransformerModel, self).__init__()\n",
    "\t\t# layers\n",
    "\t\tself.embedding = nn.Linear(input_dim, model_dim)\n",
    "\t\tencoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, batch_first=True)\n",
    "\t\tself.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\t\tself.fc = nn.Linear(model_dim, output_dim)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.embedding(x)\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = self.transformer(x)\n",
    "\t\tx = self.fc(x.mean(dim=1))\n",
    "\t\treturn x\n",
    "\t\n",
    "class EnsembleModel(nn.Module):\n",
    "\tdef __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim, type, dropout=0.1):\n",
    "\t\tsuper(EnsembleModel, self).__init__()\n",
    "\t\tself.model1 = FeedforwardModel(input_dim, output_dim, dropout)\n",
    "\t\tself.model2 = TransformerModel(input_dim, model_dim, num_heads, num_layers, output_dim, dropout)\n",
    "\t\tself.model3 = TabNetRegressor()\n",
    "\t\tself.model3.load_model(f'../models/tabnet/{type}.zip')\n",
    "\t\tself.model4 = xgb.XGBRegressor()\n",
    "\t\tself.model4.load_model(f'../models/xgboost/{type}.json')\n",
    "\t\tself.fc = nn.Linear(output_dim * 2 + 1 + 1, output_dim)\n",
    "\t\n",
    "\tdef forward(self, x, x_t):\n",
    "\t\tout1 = self.model1(x_t)\n",
    "\t\tout2 = self.model2(x_t)\n",
    "\t\tout3 = self.model3.predict(x)\n",
    "\t\tout4 = self.model4.predict(x)\n",
    "\n",
    "\t\tout3 = torch.tensor(out3, dtype=torch.float32).view(-1, 1).to(x_t.device)\n",
    "\t\tout4 = torch.tensor(out4, dtype=torch.float32).view(-1, 1).to(x_t.device)\n",
    "\n",
    "\t\tout = torch.cat((out1, out2, out3, out4), dim=1)\n",
    "\t\tout = self.fc(out)\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "\t\ttrial: optuna.Trial,\n",
    "\t\tX_train, X_test,\n",
    "\t\tX_train_t, y_train_t, X_test_t, y_test_t,\n",
    "\t\tinput_dim, output_dim, type\n",
    "\t):\n",
    "\t# Definimos los hiperparámetros a buscar\n",
    "\tnum_heads = trial.suggest_int('num_heads', 1, 8)\n",
    "\tmodel_dim = trial.suggest_int('model_dim', num_heads * 4, num_heads * 64, step=num_heads)\n",
    "\tnum_layers = trial.suggest_int('num_layers', 1, 6)\n",
    "\tdropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "\tlearning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "\tweight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
    "\tnum_epochs = trial.suggest_int('num_epochs', 10, 100)\n",
    "\n",
    "\t# model initialization \n",
    "\tmodel = EnsembleModel(input_dim, model_dim, num_heads, num_layers, output_dim, type, dropout)\n",
    "\tif DEVICE.type == 'cuda':\n",
    "\t\tmodel = model.to(DEVICE)\n",
    "\tcriterion = nn.MSELoss()\n",
    "\toptimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\t# training\n",
    "\tmodel.train()\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutput = model(X_train, X_train_t)\n",
    "\t\tloss = criterion(output, y_train_t)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t# evaluation\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tpredictions = model(X_test, X_test_t)\n",
    "\t\tval_loss = criterion(predictions, y_test_t)\n",
    "\n",
    "\t\t# trial.report(val_loss.item(), epoch+1)\n",
    "\t\t# if trial.should_prune():\n",
    "\t\t# \traise optuna.TrialPruned()\n",
    "\tprint(f\"Trial: {trial.number} - Loss: {loss.item()} - Val Loss: {val_loss.item()}\")\n",
    "\treturn val_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 25\n",
    "study_g = None\n",
    "study_st = None\n",
    "study_mm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 10:03:45,302] A new study created in memory with name: no-name-fc23a59b-9e75-40e4-b0f1-25a40a19c2be\n",
      "/home/frnk65/resource-prediction-study/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/frnk65/resource-prediction-study/venv/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "[I 2024-07-03 10:04:20,941] Trial 0 finished with value: 109.85990142822266 and parameters: {'num_heads': 7, 'model_dim': 280, 'num_layers': 5, 'dropout': 0.10823379771832098, 'learning_rate': 0.008123245085588688, 'weight_decay': 0.00314288089084011, 'num_epochs': 29}. Best is trial 0 with value: 109.85990142822266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0 - Loss: 120.29765319824219 - Val Loss: 109.85990142822266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 10:04:46,208] Trial 1 finished with value: 459.22149658203125 and parameters: {'num_heads': 2, 'model_dim': 30, 'num_layers': 2, 'dropout': 0.3099025726528951, 'learning_rate': 0.00019762189340280086, 'weight_decay': 7.476312062252303e-05, 'num_epochs': 65}. Best is trial 0 with value: 109.85990142822266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 1 - Loss: 444.8358154296875 - Val Loss: 459.22149658203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 10:05:10,622] Trial 2 finished with value: 115.1418685913086 and parameters: {'num_heads': 2, 'model_dim': 42, 'num_layers': 3, 'dropout': 0.28242799368681437, 'learning_rate': 0.0022673986523780395, 'weight_decay': 3.972110727381908e-05, 'num_epochs': 56}. Best is trial 0 with value: 109.85990142822266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 2 - Loss: 158.81756591796875 - Val Loss: 115.1418685913086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 10:06:17,991] Trial 3 finished with value: 499.9704895019531 and parameters: {'num_heads': 5, 'model_dim': 30, 'num_layers': 4, 'dropout': 0.16820964947491662, 'learning_rate': 1.5673095467235405e-05, 'weight_decay': 0.007025166339242158, 'num_epochs': 97}. Best is trial 0 with value: 109.85990142822266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 3 - Loss: 478.4620666503906 - Val Loss: 499.9704895019531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 10:06:41,584] Trial 4 finished with value: 373.4997253417969 and parameters: {'num_heads': 7, 'model_dim': 154, 'num_layers': 1, 'dropout': 0.3736932106048628, 'learning_rate': 0.00020914981329035596, 'weight_decay': 2.32335035153901e-05, 'num_epochs': 55}. Best is trial 0 with value: 109.85990142822266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 4 - Loss: 387.8373107910156 - Val Loss: 373.4997253417969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 10:07:01,964] Trial 5 finished with value: 456.6725769042969 and parameters: {'num_heads': 1, 'model_dim': 59, 'num_layers': 2, 'dropout': 0.36500891374159283, 'learning_rate': 8.612579192594876e-05, 'weight_decay': 0.00036324869566766035, 'num_epochs': 59}. Best is trial 0 with value: 109.85990142822266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 5 - Loss: 446.53472900390625 - Val Loss: 456.6725769042969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 10:07:58,894] Trial 6 finished with value: 225.4366912841797 and parameters: {'num_heads': 2, 'model_dim': 126, 'num_layers': 5, 'dropout': 0.4757995766256756, 'learning_rate': 0.004835952776465951, 'weight_decay': 0.0006218704727769079, 'num_epochs': 93}. Best is trial 0 with value: 109.85990142822266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 6 - Loss: 38.048770904541016 - Val Loss: 225.4366912841797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 10:08:24,711] Trial 7 finished with value: 483.0039367675781 and parameters: {'num_heads': 1, 'model_dim': 15, 'num_layers': 1, 'dropout': 0.23013213230530574, 'learning_rate': 0.00014656553886225324, 'weight_decay': 6.516990611177177e-05, 'num_epochs': 85}. Best is trial 0 with value: 109.85990142822266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 7 - Loss: 462.137451171875 - Val Loss: 483.0039367675781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 10:09:21,827] Trial 8 finished with value: 241.3771514892578 and parameters: {'num_heads': 3, 'model_dim': 63, 'num_layers': 4, 'dropout': 0.15636968998990508, 'learning_rate': 0.002550298070162891, 'weight_decay': 1.6736010167825783e-05, 'num_epochs': 99}. Best is trial 0 with value: 109.85990142822266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 8 - Loss: 49.231842041015625 - Val Loss: 241.3771514892578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-03 10:09:55,693] Trial 9 finished with value: 225.70355224609375 and parameters: {'num_heads': 7, 'model_dim': 112, 'num_layers': 1, 'dropout': 0.42618457138193366, 'learning_rate': 0.001319994226153501, 'weight_decay': 0.0015382308040279, 'num_epochs': 80}. Best is trial 0 with value: 109.85990142822266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 9 - Loss: 159.583984375 - Val Loss: 225.70355224609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-07-03 10:09:56,620] Trial 10 failed with parameters: {'num_heads': 8, 'model_dim': 424, 'num_layers': 6, 'dropout': 0.10718475024592788, 'learning_rate': 0.008947143993486005, 'weight_decay': 0.005284384205738849, 'num_epochs': 19} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 2.12 GiB. GPU 0 has a total capacity of 7.91 GiB of which 188.38 MiB is free. Process 102737 has 126.00 MiB memory in use. Process 72572 has 170.00 MiB memory in use. Process 74823 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 6.70 GiB memory in use. Of the allocated memory 5.21 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/frnk65/resource-prediction-study/venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_227211/4091844140.py\", line 3, in <lambda>\n",
      "    study_g.optimize(lambda trial: objective(trial,\n",
      "  File \"/tmp/ipykernel_227211/3175925706.py\", line 28, in objective\n",
      "    loss.backward()\n",
      "  File \"/home/frnk65/resource-prediction-study/venv/lib/python3.10/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/frnk65/resource-prediction-study/venv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.12 GiB. GPU 0 has a total capacity of 7.91 GiB of which 188.38 MiB is free. Process 102737 has 126.00 MiB memory in use. Process 72572 has 170.00 MiB memory in use. Process 74823 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 6.70 GiB memory in use. Of the allocated memory 5.21 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[W 2024-07-03 10:09:56,621] Trial 10 failed with value None.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.12 GiB. GPU 0 has a total capacity of 7.91 GiB of which 188.38 MiB is free. Process 102737 has 126.00 MiB memory in use. Process 72572 has 170.00 MiB memory in use. Process 74823 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 6.70 GiB memory in use. Of the allocated memory 5.21 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# configuration optuna\u001b[39;00m\n\u001b[1;32m      2\u001b[0m study_g \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mstudy_g\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mX_g_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_g_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mX_g_train_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_g_train_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_g_test_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_g_test_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeneral\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/resource-prediction-study/venv/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/resource-prediction-study/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/resource-prediction-study/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/resource-prediction-study/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/resource-prediction-study/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# configuration optuna\u001b[39;00m\n\u001b[1;32m      2\u001b[0m study_g \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39msampler)\n\u001b[0;32m----> 3\u001b[0m study_g\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mX_g_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_g_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mX_g_train_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_g_train_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_g_test_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_g_test_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeneral\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39mn_trials)\n",
      "Cell \u001b[0;32mIn[21], line 28\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, X_train, X_test, X_train_t, y_train_t, X_test_t, y_test_t, input_dim, output_dim, type)\u001b[0m\n\u001b[1;32m     26\u001b[0m \toutput \u001b[38;5;241m=\u001b[39m model(X_train, X_train_t)\n\u001b[1;32m     27\u001b[0m \tloss \u001b[38;5;241m=\u001b[39m criterion(output, y_train_t)\n\u001b[0;32m---> 28\u001b[0m \t\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \toptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# evaluation\u001b[39;00m\n",
      "File \u001b[0;32m~/resource-prediction-study/venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/resource-prediction-study/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.12 GiB. GPU 0 has a total capacity of 7.91 GiB of which 188.38 MiB is free. Process 102737 has 126.00 MiB memory in use. Process 72572 has 170.00 MiB memory in use. Process 74823 has 180.00 MiB memory in use. Including non-PyTorch memory, this process has 6.70 GiB memory in use. Of the allocated memory 5.21 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# configuration optuna\n",
    "study_g = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study_g.optimize(lambda trial: objective(trial,\n",
    "                                        X_g_train, X_g_test, \n",
    "                                        X_g_train_t, y_g_train_t, X_g_test_t, y_g_test_t,\n",
    "                                        len(features), 1, 'general'), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_folder = '../models/ensemble/'\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump(scaler_g, f'{models_folder}/scaler_g.joblib')\n",
    "#dump(scaler_st, f'{models_folder}/scaler_st.joblib')\n",
    "#dump(scaler_mm, f'{models_folder}/scaler_mm.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "torch.Size([900, 1, 16])\n",
      "MSE: 97.95959151197678 - RMSE: 9.897453789332728 - MAE: 6.771173392910427\n"
     ]
    }
   ],
   "source": [
    "# model initialization \n",
    "model_g = EnsembleModel(input_dim, model_dim, num_heads, num_layers, output_dim, type, dropout)\n",
    "if DEVICE.type == 'cuda':\n",
    "\tmodel_g = model_g.to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model_g.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "# training\n",
    "model_g.train()\n",
    "for epoch in range(num_epochs):\n",
    "\toptimizer.zero_grad()\n",
    "\toutput = model_g(X_train, X_train_t)\n",
    "\tloss = criterion(output, y_train_t)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\t# validation\n",
    "\tif (epoch+1) % 10 == 0 or epoch == num_epochs-1:\n",
    "\t\tmodel_g.eval()\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tval_predictions = model_g(X_g_test, X_g_test_t)\n",
    "\t\t\tval_loss = criterion(val_predictions, y_g_test)\n",
    "\t\tprint(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\t\tmodel_g.train()\n",
    "\n",
    "print(f\"MSE: {mse} - RMSE: {np.sqrt(mse)} - MAE: {mean_absolute_error(y_g_test, preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g.eval()\n",
    "with torch.no_grad():\n",
    "\tpreds = model_g(X_g_test_t).cpu().numpy().flatten()\n",
    "mse = mean_squared_error(y_g_test.cpu().numpy().flatten(), preds)\n",
    "print(f\"MSE: {mse} - RMSE: {np.sqrt(mse)} - MAE: {mean_absolute_error(y_g_test.cpu().numpy().flatten(), preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model_g, f'{models_folder}/general.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frnk65/resource-prediction-study/venv/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "# general models\n",
    "## TabNet\n",
    "st_tabnet = TabNetRegressor()\n",
    "st_tabnet.load_model(models_path['single_thread']['tabnet'])\n",
    "## XGBoost\n",
    "st_xgboost = xgb.XGBRegressor()\n",
    "st_xgboost.load_model(models_path['single_thread']['xgboost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_st = [\n",
    "    torch.load(models_path[\"single_thread\"][\"feedforward\"]).to(DEVICE),\n",
    "    torch.load(models_path[\"single_thread\"][\"transformer\"]).to(DEVICE),\n",
    "    st_tabnet,\n",
    "    st_xgboost\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=14, out_features=64, bias=True)\n",
      "torch.Size([675, 1, 14])\n",
      "MSE: 56.74999736758766 - RMSE: 7.533259411940336 - MAE: 4.2718147920961735\n"
     ]
    }
   ],
   "source": [
    "preds = ensemble_predict(models_st, X_st_test, X_st_test_t)\n",
    "mse = mean_squared_error(y_st_test, preds)\n",
    "print(f\"MSE: {mse} - RMSE: {np.sqrt(mse)} - MAE: {mean_absolute_error(y_st_test, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frnk65/resource-prediction-study/venv/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "# general models\n",
    "## TabNet\n",
    "mm_tabnet = TabNetRegressor()\n",
    "mm_tabnet.load_model(models_path['multi_thread']['tabnet'])\n",
    "## XGBoost\n",
    "mm_xgboost = xgb.XGBRegressor()\n",
    "mm_xgboost.load_model(models_path['multi_thread']['xgboost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_mm = [\n",
    "    torch.load(models_path[\"multi_thread\"][\"feedforward\"]).to(DEVICE),\n",
    "    torch.load(models_path[\"multi_thread\"][\"transformer\"]).to(DEVICE),\n",
    "    mm_tabnet,\n",
    "    mm_xgboost\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "torch.Size([225, 1, 16])\n",
      "MSE: 492.0993801494934 - RMSE: 22.183313101281634 - MAE: 22.010240815056694\n"
     ]
    }
   ],
   "source": [
    "preds = ensemble_predict(models_mm, X_mm_test, X_mm_test_t)\n",
    "mse = mean_squared_error(y_mm_test, preds)\n",
    "print(f\"MSE: {mse} - RMSE: {np.sqrt(mse)} - MAE: {mean_absolute_error(y_mm_test, preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_val(model, X, X_t, y):\n",
    "\tmin_instance = {\"prediction\": float('inf'), \"actual\": 0, \"index\": 0}\n",
    "\tmax_instance = {\"prediction\": 0, \"actual\": 0, \"index\": 0}\n",
    "\t\n",
    "\tpredictions = ensemble_predict(model, X, X_t)\n",
    "\tindex_min = np.argmin(np.abs(predictions - y))\n",
    "\tmin_instance[\"prediction\"] = predictions[index_min]\n",
    "\tmin_instance[\"actual\"] = y[index_min]\n",
    "\tmin_instance[\"index\"] = index_min\n",
    "\tindex_max = np.argmax(np.abs(predictions - y))\n",
    "\tmax_instance[\"prediction\"] = predictions[index_max]\n",
    "\tmax_instance[\"actual\"] = y[index_max]\n",
    "\tmax_instance[\"index\"] = index_max\n",
    "\n",
    "\treturn min_instance, max_instance, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set general model\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "torch.Size([900, 1, 16])\n",
      "Mean prediction: 23.126462936401367 | Std actual: 3.2905521392822266\n",
      "Mean actual: 27.556500000000003 | Std actual: 7.4113266524961645\n",
      "Mean Error: 6.771173392910427 | Std Error: 7.218781226434187\n",
      "---\n",
      "Min instance\n",
      "total_time                                                            24.22\n",
      "total_cpu_usage                                                        0.99\n",
      "max_ram_usage                                                     25.316406\n",
      "brand_raw                         Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz\n",
      "count                                                                    24\n",
      "l2_cache_size                                                           6.0\n",
      "l3_cache_size                                                          30.0\n",
      "l2_cache_line_size                                                      256\n",
      "l2_cache_associativity                                                    6\n",
      "benchmark                                                               TSP\n",
      "ghz_actual_friendly                                                     1.2\n",
      "ghz_advertised_friendly                                                 2.3\n",
      "total_time_target                                                      23.3\n",
      "brand_raw_target                        13th Gen Intel(R) Core(TM) i5-1335U\n",
      "count_target                                                             12\n",
      "l2_cache_size_target                                                    7.5\n",
      "l3_cache_size_target                                                   12.0\n",
      "l2_cache_line_size_target                                              1280\n",
      "l2_cache_associativity_target                                             7\n",
      "ghz_advertised_friendly_target                                        2.496\n",
      "Name: 572, dtype: object\n",
      "Min Prediction: 23.30609893798828 | Actual: 23.3 | Error: 0.0060989379882805395\n",
      "---\n",
      "Max instance\n",
      "total_time                                                            15.28\n",
      "total_cpu_usage                                                        0.99\n",
      "max_ram_usage                                                   1436.171875\n",
      "brand_raw                         Intel(R) Core(TM) i5-10300H CPU @ 2.50GHz\n",
      "count                                                                     8\n",
      "l2_cache_size                                                           1.0\n",
      "l3_cache_size                                                           8.0\n",
      "l2_cache_line_size                                                      256\n",
      "l2_cache_associativity                                                    6\n",
      "benchmark                                                               KNP\n",
      "ghz_actual_friendly                                                   2.496\n",
      "ghz_advertised_friendly                                                 2.5\n",
      "total_time_target                                                     45.91\n",
      "brand_raw_target                        13th Gen Intel(R) Core(TM) i5-1335U\n",
      "count_target                                                             12\n",
      "l2_cache_size_target                                                    7.5\n",
      "l3_cache_size_target                                                   12.0\n",
      "l2_cache_line_size_target                                              1280\n",
      "l2_cache_associativity_target                                             7\n",
      "ghz_advertised_friendly_target                                        2.496\n",
      "Name: 420, dtype: object\n",
      "Max Prediction: 20.062070846557617 | Actual: 45.91 | Error: 25.84792915344238\n"
     ]
    }
   ],
   "source": [
    "# general model\n",
    "print(\"Validation set general model\")\n",
    "min_instance, max_instance, predictions = describe_val(models_g, X_g_test, X_g_test_t, y_g_test)\n",
    "errors = np.abs(predictions - y_g_test)\n",
    "mean_error = np.mean(errors)\n",
    "std_error = np.std(errors)\n",
    "\n",
    "print(f\"Mean prediction: {np.mean(predictions)} | Std actual: {np.std(predictions)}\")\n",
    "print(f\"Mean actual: {np.mean(y_g_test)} | Std actual: {np.std(y_g_test)}\")\n",
    "print(f\"Mean Error: {mean_error} | Std Error: {std_error}\")\n",
    "print(\"---\")\n",
    "print(\"Min instance\")\n",
    "print(g_test.iloc[min_instance[\"index\"]])\n",
    "print(f\"Min Prediction: {min_instance['prediction']} | Actual: {min_instance['actual']} | Error: {abs(min_instance['prediction'] - min_instance['actual'])}\")\n",
    "print(\"---\")\n",
    "print(\"Max instance\")\n",
    "print(g_test.iloc[max_instance[\"index\"]])\n",
    "print(f\"Max Prediction: {max_instance['prediction']} | Actual: {max_instance['actual']} | Error: {abs(max_instance['prediction'] - max_instance['actual'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set single thread model\n",
      "Linear(in_features=14, out_features=64, bias=True)\n",
      "torch.Size([675, 1, 14])\n",
      "Mean prediction: 23.79733657836914 | Std actual: 3.99114727973938\n",
      "Mean actual: 24.564000000000007 | Std actual: 6.096883138128859\n",
      "Mean Error: 4.2718147920961735 | Std Error: 6.204965410831554\n",
      "Min instance\n",
      "total_time                                                           24.81\n",
      "total_cpu_usage                                                        1.0\n",
      "max_ram_usage                                                    31.253906\n",
      "brand_raw                         Intel(R) Core(TM) i5-8300H CPU @ 2.30GHz\n",
      "count                                                                    8\n",
      "l2_cache_size                                                          1.0\n",
      "l3_cache_size                                                          8.0\n",
      "l2_cache_line_size                                                     256\n",
      "l2_cache_associativity                                                   6\n",
      "benchmark                                                              TSP\n",
      "ghz_actual_friendly                                                  2.304\n",
      "ghz_advertised_friendly                                                2.3\n",
      "total_time_target                                                    22.63\n",
      "brand_raw_target                       13th Gen Intel(R) Core(TM) i5-1335U\n",
      "count_target                                                            12\n",
      "l2_cache_size_target                                                   7.5\n",
      "l3_cache_size_target                                                  12.0\n",
      "l2_cache_line_size_target                                             1280\n",
      "l2_cache_associativity_target                                            7\n",
      "ghz_advertised_friendly_target                                       2.496\n",
      "Name: 296, dtype: object\n",
      "Min Prediction: 22.629518508911133 | Actual: 22.63 | Error: 0.00048149108886619274\n",
      "---\n",
      "Max instance\n",
      "total_time                                                            15.54\n",
      "total_cpu_usage                                                        0.99\n",
      "max_ram_usage                                                   1435.285156\n",
      "brand_raw                         Intel(R) Core(TM) i5-10300H CPU @ 2.50GHz\n",
      "count                                                                     8\n",
      "l2_cache_size                                                           1.0\n",
      "l3_cache_size                                                           8.0\n",
      "l2_cache_line_size                                                      256\n",
      "l2_cache_associativity                                                    6\n",
      "benchmark                                                               KNP\n",
      "ghz_actual_friendly                                                   2.496\n",
      "ghz_advertised_friendly                                                 2.5\n",
      "total_time_target                                                     45.91\n",
      "brand_raw_target                        13th Gen Intel(R) Core(TM) i5-1335U\n",
      "count_target                                                             12\n",
      "l2_cache_size_target                                                    7.5\n",
      "l3_cache_size_target                                                   12.0\n",
      "l2_cache_line_size_target                                              1280\n",
      "l2_cache_associativity_target                                             7\n",
      "ghz_advertised_friendly_target                                        2.496\n",
      "Name: 305, dtype: object\n",
      "Max Prediction: 17.706510543823242 | Actual: 45.91 | Error: 28.203489456176754\n"
     ]
    }
   ],
   "source": [
    "# single thread model\n",
    "print(\"Validation set single thread model\")\n",
    "min_instance, max_instance, predictions = describe_val(models_st, X_st_test, X_st_test_t, y_st_test)\n",
    "errors = np.abs(predictions - y_st_test)\n",
    "mean_error = np.mean(errors)\n",
    "std_error = np.std(errors)\n",
    "\n",
    "print(f\"Mean prediction: {np.mean(predictions)} | Std actual: {np.std(predictions)}\")\n",
    "print(f\"Mean actual: {np.mean(y_st_test)} | Std actual: {np.std(y_st_test)}\")\n",
    "print(f\"Mean Error: {mean_error} | Std Error: {std_error}\")\n",
    "print(\"Min instance\")\n",
    "print(st_test.iloc[min_instance[\"index\"]])\n",
    "print(f\"Min Prediction: {min_instance['prediction']} | Actual: {min_instance['actual']} | Error: {abs(min_instance['prediction'] - min_instance['actual'])}\")\n",
    "print(\"---\")\n",
    "print(\"Max instance\")\n",
    "print(st_test.iloc[max_instance[\"index\"]])\n",
    "print(f\"Max Prediction: {max_instance['prediction']} | Actual: {max_instance['actual']} | Error: {abs(max_instance['prediction'] - max_instance['actual'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set multi thread model\n",
      "Linear(in_features=16, out_features=64, bias=True)\n",
      "torch.Size([225, 1, 16])\n",
      "Mean prediction: 14.523759841918945 | Std actual: 2.6294972896575928\n",
      "Mean actual: 36.534 | Std actual: 0.8569854141115829\n",
      "Mean Error: 22.010240815056694 | Std Error: 2.7656245972123217\n",
      "Min instance\n",
      "total_time                                                            26.65\n",
      "total_cpu_usage                                                        1.92\n",
      "max_ram_usage                                                   2333.460938\n",
      "brand_raw                         Intel(R) Xeon(R) CPU E5-2623 v3 @ 3.00GHz\n",
      "count                                                                     8\n",
      "l2_cache_size                                                           2.0\n",
      "l3_cache_size                                                          10.0\n",
      "l2_cache_line_size                                                      256\n",
      "l2_cache_associativity                                                    2\n",
      "benchmark                                                       MATRIX_MULT\n",
      "ghz_actual_friendly                                                  3.0005\n",
      "ghz_advertised_friendly                                                 3.0\n",
      "total_time_target                                                     35.58\n",
      "brand_raw_target                        13th Gen Intel(R) Core(TM) i5-1335U\n",
      "count_target                                                             12\n",
      "l2_cache_size_target                                                    7.5\n",
      "l3_cache_size_target                                                   12.0\n",
      "l2_cache_line_size_target                                              1280\n",
      "l2_cache_associativity_target                                             7\n",
      "ghz_advertised_friendly_target                                        2.496\n",
      "Name: 159, dtype: object\n",
      "Min Prediction: 19.0046443939209 | Actual: 35.58 | Error: 16.5753556060791\n",
      "---\n",
      "Max instance\n",
      "total_time                                                            3.74\n",
      "total_cpu_usage                                                      22.66\n",
      "max_ram_usage                                                  2380.378906\n",
      "brand_raw                         Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz\n",
      "count                                                                   40\n",
      "l2_cache_size                                                         40.0\n",
      "l3_cache_size                                                         27.5\n",
      "l2_cache_line_size                                                     256\n",
      "l2_cache_associativity                                                   6\n",
      "benchmark                                                      MATRIX_MULT\n",
      "ghz_actual_friendly                                                0.95592\n",
      "ghz_advertised_friendly                                                2.1\n",
      "total_time_target                                                    38.02\n",
      "brand_raw_target                       13th Gen Intel(R) Core(TM) i5-1335U\n",
      "count_target                                                            12\n",
      "l2_cache_size_target                                                   7.5\n",
      "l3_cache_size_target                                                  12.0\n",
      "l2_cache_line_size_target                                             1280\n",
      "l2_cache_associativity_target                                            7\n",
      "ghz_advertised_friendly_target                                       2.496\n",
      "Name: 195, dtype: object\n",
      "Max Prediction: 10.467986106872559 | Actual: 38.02 | Error: 27.552013893127445\n"
     ]
    }
   ],
   "source": [
    "# multi thread model\n",
    "print(\"Validation set multi thread model\")\n",
    "min_instance, max_instance, predictions = describe_val(models_mm, X_mm_test, X_mm_test_t, y_mm_test)\n",
    "errors = np.abs(predictions - y_mm_test)\n",
    "mean_error = np.mean(errors)\n",
    "std_error = np.std(errors)\n",
    "\n",
    "print(f\"Mean prediction: {np.mean(predictions)} | Std actual: {np.std(predictions)}\")\n",
    "print(f\"Mean actual: {np.mean(y_mm_test)} | Std actual: {np.std(y_mm_test)}\")\n",
    "print(f\"Mean Error: {mean_error} | Std Error: {std_error}\")\n",
    "print(\"Min instance\")\n",
    "print(mm_test.iloc[min_instance[\"index\"]])\n",
    "print(f\"Min Prediction: {min_instance['prediction']} | Actual: {min_instance['actual']} | Error: {abs(min_instance['prediction'] - min_instance['actual'])}\")\n",
    "print(\"---\")\n",
    "print(\"Max instance\")\n",
    "print(mm_test.iloc[max_instance[\"index\"]])\n",
    "print(f\"Max Prediction: {max_instance['prediction']} | Actual: {max_instance['actual']} | Error: {abs(max_instance['prediction'] - max_instance['actual'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
